{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Feature Engineering Pipeline\n",
                "\n",
                "**Goal:** Implement a robust feature engineering pipeline to prepare the data for advanced machine learning models. This pipeline includes data cleaning, missing value imputation, feature creation, and encoding.\n",
                "\n",
                "## 1. Setup & Data Loading\n",
                "We load the training and test datasets and combine them to ensure consistent preprocessing (e.g., same One-Hot Encoding columns)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "13861f36",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Combined Shape: (150000, 29)\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import statistics as mode\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "# Load Data\n",
                "train = pd.read_csv(\"../../data/raw/train.csv\", low_memory=False)\n",
                "test = pd.read_csv(\"../../data/raw/test.csv\", low_memory=False)\n",
                "\n",
                "# Combine for consistent preprocessing (splitting back later)\n",
                "train['is_train'] = 1\n",
                "test['is_train'] = 0\n",
                "df = pd.concat([train, test], ignore_index=True)\n",
                "\n",
                "print(f\"Combined Shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f453e5dc",
            "metadata": {},
            "source": [
                "## 2. Data Cleaning & Type Conversion\n",
                "Many numerical columns contain special characters (underscores, commas) or are stored as strings. We clean these to convert them to proper float format.\n",
                "<br>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "fa0f4ba2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Age NaNs before group imputation: 4177\n",
                        "Cleaning complete. Checking dtypes:\n",
                        "Age                        float64\n",
                        "Annual_Income              float64\n",
                        "Num_of_Loan                float64\n",
                        "Num_of_Delayed_Payment     float64\n",
                        "Changed_Credit_Limit       float64\n",
                        "Outstanding_Debt           float64\n",
                        "Amount_invested_monthly    float64\n",
                        "Monthly_Balance            float64\n",
                        "dtype: object\n"
                    ]
                }
            ],
            "source": [
                "# Helper function to clean numerical columns\n",
                "def clean_numeric(x):\n",
                "    if pd.isna(x): return np.nan\n",
                "    if isinstance(x, (int, float)): return x\n",
                "    # Remove underscores and other non-numeric chars (keep decimal point and negative sign)\n",
                "    x = str(x).replace('_', '').replace(',', '').strip()\n",
                "    if x == '': return np.nan\n",
                "    try:\n",
                "        return float(x)\n",
                "    except ValueError:\n",
                "        return np.nan\n",
                "\n",
                "cols_to_clean = ['Age', 'Annual_Income', 'Num_of_Loan', 'Num_of_Delayed_Payment', \n",
                "                 'Changed_Credit_Limit', 'Outstanding_Debt', 'Amount_invested_monthly', \n",
                "                 'Monthly_Balance']\n",
                "\n",
                "for col in cols_to_clean:\n",
                "    df[col] = df[col].apply(clean_numeric)\n",
                "\n",
                "# Handle specific outliers/invalid values immediately after conversion\n",
                "df.loc[(df['Age'] > 100) | (df['Age'] < 0), 'Age'] = np.nan  # Invalid ages\n",
                "print(f\"Age NaNs before group imputation: {df['Age'].isna().sum()}\")\n",
                "\n",
                "print(\"Cleaning complete. Checking dtypes:\")\n",
                "print(df[cols_to_clean].dtypes)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8d8e604a",
            "metadata": {},
            "source": [
                "## 3. Feature Extraction (Creating New Features)\n",
                "We extract meaningful signals from complex columns:\n",
                "*   **Credit History Age:** Converted from \"X Years Y Months\" string to total months.\n",
                "*   **Type of Loan:** Split into binary flags for common loan types (Auto, Mortgage, etc.) to capture specific risk profiles.\n",
                "*   **Debt to Income Ratio:** A classic financial risk metric."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "7419672c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Feature extraction complete.\n"
                    ]
                }
            ],
            "source": [
                "# 3.1 Credit History Age -> Months\n",
                "def parse_credit_history(x):\n",
                "    if pd.isna(x): return np.nan\n",
                "\n",
                "    years = re.search(r'(\\d+)\\s*Years?', str(x))\n",
                "    months = re.search(r'(\\d+)\\s*Months?', str(x))\n",
                "    \n",
                "    total = 0\n",
                "    if years: total += int(years.group(1)) * 12\n",
                "    if months: total += int(months.group(1))\n",
                "    return total\n",
                "\n",
                "df['Credit_History_Months'] = df['Credit_History_Age'].apply(parse_credit_history)\n",
                "\n",
                "# 3.2 Type of Loan -> One-Hot & Count\n",
                "# Fill NaN with 'Unknown' first\n",
                "df['Type_of_Loan'] = df['Type_of_Loan'].fillna('Unknown')\n",
                "\n",
                "# Count loans\n",
                "df['Loan_Count_Calculated'] = df['Type_of_Loan'].apply(lambda x: len(x.split(', ')) if x != 'Unknown' else 0)\n",
                "\n",
                "# One-Hot Encode Top Loans\n",
                "top_loans = ['Auto Loan', 'Credit-Builder Loan', 'Personal Loan', 'Home Equity Loan', \n",
                "             'Mortgage Loan', 'Student Loan', 'Debt Consolidation Loan', 'Payday Loan']\n",
                "\n",
                "for loan in top_loans:\n",
                "    df[f'Loan_{loan.replace(\" \", \"_\")}'] = df['Type_of_Loan'].apply(lambda x: 1 if loan in x else 0)\n",
                "\n",
                "# 3.3 Debt to Income Ratio\n",
                "# Handle division by zero or NaN\n",
                "df['Debt_to_Income_Ratio'] = df['Outstanding_Debt'] / df['Annual_Income']\n",
                "df['Debt_to_Income_Ratio'] = df['Debt_to_Income_Ratio'].replace([np.inf, -np.inf], np.nan)\n",
                "\n",
                "# 3.4 Payment Behaviour Cleaning\n",
                "df['Payment_Behaviour'] = df['Payment_Behaviour'].replace('!@9#%8', 'Unknown')\n",
                "\n",
                "\n",
                "# 3.5 Loan interaction features \n",
                "\n",
                "# Interaction: DTI × Loan Count\n",
                "df['DTI_x_LoanCount'] = df['Debt_to_Income_Ratio'] * df['Loan_Count_Calculated']\n",
                "\n",
                "# Debt per loan\n",
                "df['Debt_Per_Loan'] = df['Outstanding_Debt'] / df['Loan_Count_Calculated'].replace(0, np.nan)\n",
                "\n",
                "# Installment-to-income\n",
                "df['Installment_to_Income'] = df['Monthly_Inhand_Salary'] / df['Total_EMI_per_month'].replace(0, np.nan)\n",
                "\n",
                "# Delays per loan\n",
                "df['Delayed_Per_Loan'] = df['Num_of_Delayed_Payment'] / df['Loan_Count_Calculated'].replace(0, np.nan)\n",
                "\n",
                "print(\"Feature extraction complete.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "970d82e5",
            "metadata": {},
            "source": [
                "## 4. Imputation (Handling Missing Values)\n",
                "We use specific strategies for different column types:\n",
                "*   **Salary:** Median imputation grouped by Occupation (more accurate than global median).\n",
                "*   **Delayed Payments:** Assume 0 if missing (conservative approach).\n",
                "*   **Others:** Standard Median/Mode imputation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "e34bf284",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Imputation complete.\n"
                    ]
                }
            ],
            "source": [
                "# 4.1 Monthly_Inhand_Salary: Median grouped by Occupation\n",
                "df['Monthly_Inhand_Salary'] = df.groupby('Occupation')['Monthly_Inhand_Salary'].transform(lambda x: x.fillna(x.median()))\n",
                "# Fill remaining (if any occupation has all NaNs) with global median\n",
                "df['Monthly_Inhand_Salary'] = df['Monthly_Inhand_Salary'].fillna(df['Monthly_Inhand_Salary'].median())\n",
                "\n",
                "# 4.2 Num_of_Delayed_Payment: Assume 0 if missing\n",
                "df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].fillna(0)\n",
                "\n",
                "# 4.3 Other Numerical: Median\n",
                "num_cols = df.select_dtypes(include=[np.number]).columns\n",
                "imputer = SimpleImputer(strategy='median')\n",
                "df[num_cols] = imputer.fit_transform(df[num_cols])\n",
                "\n",
                "# 4.4 Categorical: Mode/Constant\n",
                "cat_cols = df.select_dtypes(include=['object']).columns\n",
                "exclude = ['Credit_Score', 'ID', 'Customer_ID', 'Name', 'SSN', 'is_train']\n",
                "cat_cols = [c for c in cat_cols if c not in exclude]\n",
                "\n",
                "for col in cat_cols:\n",
                "    df[col] = df[col].fillna(df[col].mode()[0])\n",
                "\n",
                "print(\"Imputation complete.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "99290b39",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "NUMERIC COLUMNS:\n",
                        "['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Total_EMI_per_month', 'Amount_invested_monthly', 'Monthly_Balance', 'is_train', 'Credit_History_Months', 'Loan_Count_Calculated', 'Loan_Auto_Loan', 'Loan_Credit-Builder_Loan', 'Loan_Personal_Loan', 'Loan_Home_Equity_Loan', 'Loan_Mortgage_Loan', 'Loan_Student_Loan', 'Loan_Debt_Consolidation_Loan', 'Loan_Payday_Loan', 'Debt_to_Income_Ratio', 'DTI_x_LoanCount', 'Debt_Per_Loan', 'Installment_to_Income', 'Delayed_Per_Loan']\n",
                        "\n",
                        "CATEGORICAL COLUMNS:\n",
                        "['ID', 'Customer_ID', 'Month', 'Name', 'SSN', 'Occupation', 'Type_of_Loan', 'Credit_Mix', 'Credit_History_Age', 'Payment_of_Min_Amount', 'Payment_Behaviour', 'Credit_Score']\n"
                    ]
                }
            ],
            "source": [
                "# see the existed cols datatypes\n",
                "print(\"NUMERIC COLUMNS:\")\n",
                "print(df.select_dtypes(include=[np.number]).columns.tolist())\n",
                "\n",
                "print(\"\\nCATEGORICAL COLUMNS:\")\n",
                "print(df.select_dtypes(include=['object']).columns.tolist())\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ddf49945",
            "metadata": {},
            "source": [
                "<h3>Customer-Level Aggregation</h3>\n",
                "<p>The dataset contains multiple monthly rows per customer, so we merge them into a single record to avoid duplication and leakage.</p>\n",
                "\n",
                "<ul>\n",
                "  <li><b>Stable numeric fields</b> (Age, Num_Bank_Accounts, loan flags): take the <b>first</b></li>\n",
                "  <li><b>Monthly-changing numeric fields</b> (Income, Balance, DTI, EMI): take the <b>mean</b></li>\n",
                "  <li><b>Count fields</b> (Delayed payments, inquiries, loan count): take the <b>sum</b></li>\n",
                "  <li><b>Categorical behaviour</b> (Payment Behaviour, Credit Mix): take the <b>mode</b></li>\n",
                "  <li><b>Identity fields</b> (Name, SSN, Occupation): take the <b>first</b></li>\n",
                "  <li><b>Target (Credit Score)</b>: take the <b>mode</b></li>\n",
                "</ul>\n",
                "\n",
                "<p>This produces one clean row per customer, ready for modeling.</p>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "id": "159cb8f9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BEFORE AGGREGATION:\n",
                        "Total rows in df: 150000\n",
                        "Train rows (is_train=1): 100000\n",
                        "Test rows (is_train=0): 50000\n",
                        "is_train unique values: [1. 0.]\n",
                        "\n",
                        "Data shape by is_train:\n",
                        "Train data shape: (100000, 44)\n",
                        "Test data shape: (50000, 44)\n",
                        "\n",
                        "Customer_ID distribution:\n",
                        "Unique Customer IDs in train: 12500\n",
                        "Unique Customer IDs in test: 12500\n",
                        "Train Customer_ID samples: ['CUS_0xd40', 'CUS_0xd40', 'CUS_0xd40', 'CUS_0xd40', 'CUS_0xd40']\n",
                        "Test Customer_ID samples: ['CUS_0xd40', 'CUS_0xd40', 'CUS_0xd40', 'CUS_0xd40', 'CUS_0x21b1']\n"
                    ]
                }
            ],
            "source": [
                "# DIAGNOSTIC: Check is_train distribution before aggregation\n",
                "print(\"BEFORE AGGREGATION:\")\n",
                "print(f\"Total rows in df: {len(df)}\")\n",
                "print(f\"Train rows (is_train=1): {(df['is_train'] == 1).sum()}\")\n",
                "print(f\"Test rows (is_train=0): {(df['is_train'] == 0).sum()}\")\n",
                "print(f\"is_train unique values: {df['is_train'].unique()}\")\n",
                "\n",
                "print(\"\\nData shape by is_train:\")\n",
                "print(f\"Train data shape: {df[df['is_train'] == 1].shape}\")\n",
                "print(f\"Test data shape: {df[df['is_train'] == 0].shape}\")\n",
                "\n",
                "print(\"\\nCustomer_ID distribution:\")\n",
                "print(f\"Unique Customer IDs in train: {df[df['is_train'] == 1]['Customer_ID'].nunique()}\")\n",
                "print(f\"Unique Customer IDs in test: {df[df['is_train'] == 0]['Customer_ID'].nunique()}\")\n",
                "print(f\"Train Customer_ID samples: {df[df['is_train'] == 1]['Customer_ID'].head().tolist()}\")\n",
                "print(f\"Test Customer_ID samples: {df[df['is_train'] == 0]['Customer_ID'].head().tolist()}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "id": "2eb3903c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting customer-level aggregation...\n",
                        "Train raw: (100000, 45), Test raw: (50000, 45)\n",
                        "Aggregated Train Shape: (12500, 37)\n",
                        "Aggregated Test Shape: (12500, 37)\n",
                        "Train has Credit_Score: True\n",
                        "Test has Credit_Score: True\n",
                        "Customer-level aggregation complete.\n"
                    ]
                }
            ],
            "source": [
                "# 4.5. Customer-Level Aggregation\n",
                "print(\"Starting customer-level aggregation...\")\n",
                "\n",
                "# Ensure is_train is float for proper filtering\n",
                "df['is_train'] = df['is_train'].astype(float)\n",
                "\n",
                "# Convert Credit_History_Age to months for proper aggregation\n",
                "def age_to_months(age_str):\n",
                "    if isinstance(age_str, str):\n",
                "        y, m = age_str.replace(\" Years\", \"\").replace(\" Months\", \"\").split(\" and \")\n",
                "        return int(y) * 12 + int(m)\n",
                "    return None\n",
                "\n",
                "df[\"Credit_History_Months_Parsed\"] = df[\"Credit_History_Age\"].apply(age_to_months)\n",
                "\n",
                "\n",
                "# IMPORTANT: Split FIRST, then aggregate separately\n",
                "# This prevents mixing train and test data for the same customer\n",
                "train_raw = df[df['is_train'] == 1.0].copy()\n",
                "test_raw = df[df['is_train'] == 0.0].copy()\n",
                "\n",
                "print(f\"Train raw: {train_raw.shape}, Test raw: {test_raw.shape}\")\n",
                "\n",
                "# Helper function to safely get mode\n",
                "def safe_mode(x):\n",
                "    mode_vals = x.mode()\n",
                "    return mode_vals.iloc[0] if len(mode_vals) > 0 else x.iloc[0]\n",
                "\n",
                "# Define aggregation rules\n",
                "agg_dict = {\n",
                "    # Constant attributes\n",
                "    \"Name\": \"first\",\n",
                "    \"Age\": \"first\",\n",
                "    \"SSN\": \"first\",\n",
                "    \"Occupation\": \"first\",\n",
                "    \"Credit_Score\": safe_mode,\n",
                "\n",
                "    # Rarely changing - mode safer than first\n",
                "    \"Num_Bank_Accounts\": safe_mode,\n",
                "    \"Num_Credit_Card\": safe_mode,\n",
                "    \"Credit_Mix\": safe_mode,\n",
                "    \"Payment_of_Min_Amount\": safe_mode,\n",
                "    \"Payment_Behaviour\": safe_mode,\n",
                "\n",
                "    # Event-like values → SUM\n",
                "    \"Delay_from_due_date\": \"sum\",\n",
                "    \"Num_of_Delayed_Payment\": \"sum\",\n",
                "    \"Num_of_Loan\": \"sum\",\n",
                "    \"Num_Credit_Inquiries\": \"sum\",\n",
                "\n",
                "    # Smooth numeric fluctuations → MEAN\n",
                "    \"Annual_Income\": \"mean\",\n",
                "    \"Monthly_Inhand_Salary\": \"mean\",\n",
                "    \"Interest_Rate\": \"mean\",\n",
                "    \"Outstanding_Debt\": \"mean\",\n",
                "    \"Credit_Utilization_Ratio\": \"mean\",\n",
                "    \"Monthly_Balance\": \"mean\",\n",
                "    \"Total_EMI_per_month\": \"mean\",\n",
                "    \"Amount_invested_monthly\": \"mean\",\n",
                "    \"Installment_to_Income\": \"mean\",\n",
                "    \"Delayed_Per_Loan\": \"mean\",\n",
                "    \"Debt_to_Income_Ratio\": \"mean\",\n",
                "    \"DTI_x_LoanCount\": \"mean\",\n",
                "    \"Debt_Per_Loan\": \"mean\",\n",
                "\n",
                "    # Loan count and loan dummy columns → FIRST\n",
                "    \"Loan_Count_Calculated\": \"first\",\n",
                "    \"Loan_Auto_Loan\": \"first\",\n",
                "    \"Loan_Credit-Builder_Loan\": \"first\",\n",
                "    \"Loan_Personal_Loan\": \"first\",\n",
                "    \"Loan_Home_Equity_Loan\": \"first\",\n",
                "    \"Loan_Mortgage_Loan\": \"first\",\n",
                "    \"Loan_Student_Loan\": \"first\",\n",
                "    \"Loan_Debt_Consolidation_Loan\": \"first\",\n",
                "    \"Loan_Payday_Loan\": \"first\",\n",
                "\n",
                "    # Months parsed\n",
                "    \"Credit_History_Months_Parsed\": \"max\",\n",
                "}\n",
                "\n",
                "# Aggregate separately for train and test\n",
                "train_agg = train_raw.groupby(\"Customer_ID\").agg(agg_dict).reset_index()\n",
                "test_agg = test_raw.groupby(\"Customer_ID\").agg(agg_dict).reset_index()\n",
                "\n",
                "# Reconstruct Credit_History_Age for both\n",
                "for df_temp in [train_agg, test_agg]:\n",
                "    df_temp[\"Credit_History_Age\"] = (\n",
                "        df_temp[\"Credit_History_Months_Parsed\"] // 12\n",
                "    ).astype(int).astype(str) + \" Years and \" + (\n",
                "        df_temp[\"Credit_History_Months_Parsed\"] % 12\n",
                "    ).astype(int).astype(str) + \" Months\"\n",
                "    df_temp.drop(columns=[\"Credit_History_Months_Parsed\", \"Name\"], inplace=True)\n",
                "\n",
                "print(f\"Aggregated Train Shape: {train_agg.shape}\")\n",
                "print(f\"Aggregated Test Shape: {test_agg.shape}\")\n",
                "print(f\"Train has Credit_Score: {'Credit_Score' in train_agg.columns}\")\n",
                "print(f\"Test has Credit_Score: {'Credit_Score' in test_agg.columns}\")\n",
                "print(\"Customer-level aggregation complete.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a52df72f",
            "metadata": {},
            "source": [
                "## 5. Outlier Treatment & Transformations\n",
                "*   **Clipping:** Cap extreme values in `Num_of_Delayed_Payment` to reduce noise.\n",
                "*   **Log Transform:** Apply to `Annual_Income` to handle skewness."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "e27b57fd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total rows after transformation: 25000\n",
                        "Train rows: 12500\n",
                        "Test rows: 12500\n",
                        "Transformations complete.\n"
                    ]
                }
            ],
            "source": [
                "# 5.1 Clipping\n",
                "# Combine train + test for consistent processing\n",
                "df_proc = pd.concat([train_agg, test_agg], axis=0, ignore_index=True)\n",
                "df_proc['_is_train'] = [1] * len(train_agg) + [0] * len(test_agg)  # Track which is train\n",
                "\n",
                "# Clip Num_of_Delayed_Payment at 99th percentile\n",
                "upper_limit = df_proc['Num_of_Delayed_Payment'].quantile(0.99)\n",
                "df_proc['Num_of_Delayed_Payment'] = df_proc['Num_of_Delayed_Payment'].clip(upper=upper_limit)\n",
                "\n",
                "# 5.2 Log Transform Annual_Income\n",
                "# Add small constant to avoid log(0)\n",
                "df_proc['Log_Annual_Income'] = np.log1p(df_proc['Annual_Income'])\n",
                "\n",
                "print(f\"Total rows after transformation: {len(df_proc)}\")\n",
                "print(f\"Train rows: {(df_proc['_is_train'] == 1).sum()}\")\n",
                "print(f\"Test rows: {(df_proc['_is_train'] == 0).sum()}\")\n",
                "print(\"Transformations complete.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "788a8977",
            "metadata": {},
            "source": [
                "## 6. Encoding & Scaling\n",
                "We convert categorical data into numerical format:\n",
                "*   **Ordinal Encoding:** For `Credit_Mix` (Bad < Standard < Good).\n",
                "*   **Cyclical Encoding:** For `Month` (preserving Jan-Dec continuity).\n",
                "*   **One-Hot Encoding:** For other categorical features.\n",
                "*   **Scaling:** Standardize numerical features for model stability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "id": "7c8629a2",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(16,\n",
                            " array(['Lawyer', 'Mechanic', 'Media_Manager', 'Doctor', 'Journalist',\n",
                            "        'Accountant', 'Manager', 'Entrepreneur', 'Scientist', 'Architect',\n",
                            "        'Teacher', '_______', 'Writer', 'Developer', 'Musician',\n",
                            "        'Engineer'], dtype=object))"
                        ]
                    },
                    "execution_count": 45,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df_proc['Occupation'].nunique(), df_proc['Occupation'].unique()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "id": "a7353c44",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "NaN count before scaling: 12500\n",
                        "Remaining NaN columns: ['Credit_Score']\n",
                        "⚠️  Note: Scaling is NOT applied to preserve tree model performance\n",
                        "    Scaling can be applied selectively for linear models in 04_model_optimization.ipynb\n",
                        "Processed Train Shape: (12500, 54)\n",
                        "Processed Test Shape: (12500, 53)\n",
                        "Train non-null Credit_Score: 12500\n",
                        "Train NaNs: 0\n",
                        "Test NaNs: 0\n",
                        "Processed data saved to data/processed/\n"
                    ]
                }
            ],
            "source": [
                "# 6.1 Ordinal Encoding: Credit_Mix (if it still exists as object type)\n",
                "if 'Credit_Mix' in df_proc.columns and df_proc['Credit_Mix'].dtype == 'object':\n",
                "    df_proc['Credit_Mix'] = df_proc['Credit_Mix'].apply(\n",
                "        lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x\n",
                "    )\n",
                "    mix_mapping = {'Bad': 0, 'Standard': 1, 'Good': 2}\n",
                "    df_proc['Credit_Mix_Ordinal'] = df_proc['Credit_Mix'].map(mix_mapping)\n",
                "\n",
                "# 6.2 Drop Columns (only if they exist)\n",
                "drop_cols = ['SSN', 'Credit_History_Age', 'Credit_Mix', 'Annual_Income', 'Customer_ID']\n",
                "existing_drop = [col for col in drop_cols if col in df_proc.columns]\n",
                "df_proc = df_proc.drop(columns=existing_drop, errors='ignore')\n",
                " \n",
                "# Fix all array/list object columns\n",
                "for col in df_proc.select_dtypes(include=['object']).columns:\n",
                "    if col not in ['_is_train', 'Credit_Score']:\n",
                "        df_proc[col] = df_proc[col].apply(\n",
                "            lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x\n",
                "        )\n",
                "\n",
                "# Fill remaining NaNs before encoding\n",
                "numeric_cols = df_proc.select_dtypes(include=[np.number]).columns\n",
                "df_proc[numeric_cols] = df_proc[numeric_cols].fillna(df_proc[numeric_cols].median())\n",
                "\n",
                "# 6.3 Label Encode Target (only for train rows)\n",
                "le = LabelEncoder()\n",
                "mask_train = df_proc['_is_train'] == 1\n",
                "if 'Credit_Score' in df_proc.columns:\n",
                "    df_proc.loc[mask_train, 'Credit_Score'] = le.fit_transform(df_proc.loc[mask_train, 'Credit_Score'].astype(str))\n",
                "\n",
                "# 6.4 One-Hot Encode remaining categoricals\n",
                "cat_cols_final = df_proc.select_dtypes(include=['object']).columns\n",
                "cat_cols_final = [c for c in cat_cols_final if c not in ['Credit_Score', '_is_train']]\n",
                "if len(cat_cols_final) > 0:\n",
                "    df_proc = pd.get_dummies(df_proc, columns=cat_cols_final, drop_first=True)\n",
                "\n",
                "# Verify no NaNs remain\n",
                "print(f\"NaN count before scaling: {df_proc.isna().sum().sum()}\")\n",
                "if df_proc.isna().sum().sum() > 0:\n",
                "    print(\"Remaining NaN columns:\", df_proc.columns[df_proc.isna().any()].tolist())\n",
                "\n",
                "# 6.5 DO NOT SCALE - Tree-based models don't benefit from scaling\n",
                "# Scaling is skipped here because:\n",
                "# - Random Forest, XGBoost are tree-based and invariant to feature scaling\n",
                "# - Scaling will be applied separately for linear models if needed\n",
                "print(\"⚠️  Note: Scaling is NOT applied to preserve tree model performance\")\n",
                "print(\"    Scaling can be applied selectively for linear models in 04_model_optimization.ipynb\")\n",
                "\n",
                "# Split back to Train/Test\n",
                "train_proc = df_proc[df_proc['_is_train'] == 1].drop(columns=['_is_train']).copy()\n",
                "test_proc = df_proc[df_proc['_is_train'] == 0].drop(columns=['_is_train']).copy()\n",
                "if 'Credit_Score' in test_proc.columns:\n",
                "    test_proc = test_proc.drop(columns=['Credit_Score'])\n",
                "\n",
                "print(f\"Processed Train Shape: {train_proc.shape}\")\n",
                "print(f\"Processed Test Shape: {test_proc.shape}\")\n",
                "print(f\"Train non-null Credit_Score: {train_proc['Credit_Score'].notna().sum()}\")\n",
                "print(f\"Train NaNs: {train_proc.isna().sum().sum()}\")\n",
                "print(f\"Test NaNs: {test_proc.isna().sum().sum()}\")\n",
                "\n",
                "# Save processed data\n",
                "train_proc.to_csv('../../data/processed/train_processed.csv', index=False)\n",
                "test_proc.to_csv('../../data/processed/test_processed.csv', index=False)\n",
                "print(\"Processed data saved to data/processed/\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eff4e5af",
            "metadata": {},
            "source": [
                "## 7. Linear Model Check (Logistic Regression)\n",
                "We first check performance with a linear model. We expect this to drop compared to the baseline because we've added complexity (One-Hot Encoding, interactions) that a simple linear model might struggle to capture without regularization or feature selection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "id": "c1c79491",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running Logistic Regression Check...\n",
                        "Logistic Regression Accuracy (with scaling): 0.6540\n"
                    ]
                }
            ],
            "source": [
                "# Prepare Data for Checks\n",
                "X = train_proc.drop('Credit_Score', axis=1)\n",
                "y = train_proc['Credit_Score'].astype(int)\n",
                "\n",
                "# Split\n",
                "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1907, stratify=y)\n",
                "\n",
                "# LOGISTIC REGRESSION CHECK\n",
                "# Scale features only for Logistic Regression (linear models need scaling)\n",
                "scaler_lr = StandardScaler()\n",
                "X_train_scaled = scaler_lr.fit_transform(X_train)\n",
                "X_val_scaled = scaler_lr.transform(X_val)\n",
                "\n",
                "print(\"Running Logistic Regression Check...\")\n",
                "lr = LogisticRegression(max_iter=1000, random_state=1907)\n",
                "lr.fit(X_train_scaled, y_train)\n",
                "y_pred_lr = lr.predict(X_val_scaled)\n",
                "\n",
                "acc_lr = accuracy_score(y_val, y_pred_lr)\n",
                "print(f\"Logistic Regression Accuracy (with scaling): {acc_lr:.4f}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "394af2c8",
            "metadata": {},
            "source": [
                "## 8. Non-Linear Model Check (Random Forest)\n",
                "Now we check with a Random Forest. This model can handle non-linear relationships and interactions much better. If this score is high, it confirms our features are good but need a non-linear model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "id": "b0171ad8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running Quick Score Check (Random Forest)...\n",
                        "Random Forest Quick Check Accuracy: 0.7340\n",
                        "\n",
                        "Classification Report (Random Forest):\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.59      0.84      0.69       501\n",
                        "           1       0.73      0.81      0.77       832\n",
                        "           2       0.86      0.63      0.73      1167\n",
                        "\n",
                        "    accuracy                           0.73      2500\n",
                        "   macro avg       0.73      0.76      0.73      2500\n",
                        "weighted avg       0.76      0.73      0.73      2500\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Quick Model (Random Forest)\n",
                "print(\"Running Quick Score Check (Random Forest)...\")\n",
                "rf_quick = RandomForestClassifier(n_estimators=500,\n",
                "    max_depth=10,\n",
                "    min_samples_split=5,\n",
                "    min_samples_leaf=2,\n",
                "    max_features='sqrt',\n",
                "    n_jobs=-1,\n",
                "class_weight='balanced', oob_score=True, random_state=1907) \n",
                "rf_quick.fit(X_train, y_train)\n",
                "y_pred = rf_quick.predict(X_val)\n",
                "\n",
                "acc = accuracy_score(y_val, y_pred)\n",
                "print(f\"Random Forest Quick Check Accuracy: {acc:.4f}\")\n",
                "print(\"\\nClassification Report (Random Forest):\")\n",
                "print(classification_report(y_val, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9f0a6042",
            "metadata": {},
            "source": [
                "## 9. Conclusion & Next Steps\n",
                "\n",
                "### Performance Summary\n",
                "\n",
                "| Model | Accuracy | Notes |\n",
                "|-------|----------|-------|\n",
                "| **Baseline** (02_baseline_model.ipynb) | 72% | Simple logistic regression on raw features |\n",
                "| **Logistic Regression** (with scaled features) | 65.44% | Complex feature space hurts linear models |\n",
                "| **Random Forest** (with hyperparameter tuning) | **73.40%** | ✅ Outperforms baseline by 1.4% |\n",
                "\n",
                "### Why Linear Models Struggle with Complex Features\n",
                "The Logistic Regression accuracy **dropped to 65.44%** despite advanced feature engineering. This reveals a key insight:\n",
                "\n",
                "1. **Feature interactions are non-linear**: Our engineered features (DTI × LoanCount, Debt_Per_Loan, Installment_to_Income) contain complex relationships that a linear decision boundary cannot capture.\n",
                "2. **One-Hot Encoding creates sparsity**: Categorical feature expansion (Occupation, Payment_Behaviour) in high dimensions reduces linear model effectiveness.\n",
                "3. **Dimensionality challenge**: With 54 features, linear models are prone to overfitting without aggressive regularization.\n",
                "\n",
                "### Why Tree-Based Models Excel\n",
                "The **Random Forest achieved 73.40% accuracy**, exceeding the baseline by 1.4 points:\n",
                "\n",
                "1. **Non-linear decision boundaries**: Trees naturally capture feature interactions without explicit engineering.\n",
                "2. **Feature importance**: Random Forest can identify which engineered features are truly valuable (this analysis will be critical in the next notebook).\n",
                "3. **Balanced class performance**: \n",
                "   - Class 0 (Poor): 84% recall → Catches risky customers\n",
                "   - Class 1 (Standard): 81% recall → Balanced performance\n",
                "   - Class 2 (Good): 63% recall → Identifies creditworthy customers\n",
                "4. **Robustness**: Hyperparameter tuning (max_depth=10, balanced_class_weight) improved generalization.\n",
                "\n",
                "### Key Learnings\n",
                "\n",
                "✅ **Engineering matters**: Feature creation (loan interactions, financial ratios) provides the signal.\n",
                "✅ **Model selection matters**: Tree-based models unlock this signal better than linear models.\n",
                "✅ **Trade-offs exist**: We gain 1.4% accuracy but lose interpretability compared to the baseline.\n",
                "\n",
                "### Next Step: Model Optimization (`04_model_optimization.ipynb`)\n",
                "\n",
                "We will now proceed to the optimization phase where we will:\n",
                "\n",
                "1. **Train XGBoost** alongside Random Forest for comparison (gradient boosting often outperforms bagging)\n",
                "2. **Rigorous Cross-Validation** with stratified k-fold to ensure the 73.4% accuracy is stable across data splits\n",
                "3. **Feature Importance Analysis** to answer: Which of our engineered features drive the predictions?\n",
                "4. **Hyperparameter Grid Search** to find the optimal trade-off between bias and variance\n",
                "5. **Class-wise analysis** to ensure good performance on all credit score classes\n",
                "6. **Final ensemble strategy** to combine models for maximum robustness\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv (3.12.8)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
