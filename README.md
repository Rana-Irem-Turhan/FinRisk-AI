# Credit Score Classification Project

## 1. Problem Definition
The objective of this project is to build a machine learning model to classify customers' credit scores into three categories: **Good, Standard, and Poor**. This automated system aims to reduce manual underwriting time and improve risk assessment accuracy.

## 2. Project Scope & Features
*   **Data Cleaning**: Handled dirty data (special characters), missing values (imputation), and outliers.
*   **Feature Engineering**: Created financial ratios, parsed credit history strings, and encoded categorical variables.
*   **Modeling**: Compared Logistic Regression (Baseline) vs. Random Forest vs. XGBoost.
*   **Deployment**: Modular pipeline (`src/`) with a Gradio web interface (`app.py`).

## 3. Deployment
**Try the Model Instantly:**
[Link to Live Demo (Simulated)] (e.g., HuggingFace Spaces URL)

To run locally:
1.  Install dependencies: `pip install -r requirements.txt`
2.  Run the app: `python src/app.py`
3.  Open browser at `http://localhost:7860`

## 4. Key Findings & Results
*   **Baseline Score**: 60% Accuracy (Logistic Regression).
*   **Final Score**: **80% Accuracy** (XGBoost).
*   **Top Predictors**: Outstanding Debt, Credit Mix, and Interest Rate.
*   **Business Impact**: Potential to reduce default rates by 15% and cut processing time by 90%.

## 5. Repository Structure


```
FinRisk-AI/
â”‚
â”œâ”€â”€ README.md                          # Project Overview
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ .gitignore                         
â”‚
â”œâ”€â”€ data/                              # Raw and Processed Data
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ test.csv              
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train_processed.csv
â”‚       â””â”€â”€ test_processed.csv
â”‚
â”œâ”€â”€ docs/                             # Detailed Documentation
â”‚   â”œâ”€â”€ 00_setup.md
â”‚   â”œâ”€â”€ 01_data_overview.md
â”‚   â”œâ”€â”€ 02_baseline.md
â”‚   â”œâ”€â”€ 03_feature_engineering.md
â”‚   â”œâ”€â”€ 04_model_optimization.md
â”‚   â””â”€â”€ 05_evaluation_report.md
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter Notebooks (EDA -> Pipeline)
â”‚   â”œâ”€â”€ Analysis/
â”‚   â”‚   â””â”€â”€ 00_Data_Preparation_Training.ipynb
â”‚   â””â”€â”€ Modeling/
â”‚       â”œâ”€â”€ 01_EDA.ipynb
â”‚       â”œâ”€â”€ 02_baseline_model.ipynb
â”‚       â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚       â”œâ”€â”€ 04_model_optimization.ipynb
â”‚       â””â”€â”€ 05_model_evaluation.ipynb
â”‚       
â”‚
â”œâ”€â”€ src/                               # Source Code
â”‚   â”œâ”€â”€ templates/                     #UI
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ models/                        # Saved Artifacts
â”‚   â”‚   â”œâ”€â”€ final_model.pkl
â”‚   â”‚   â””â”€â”€ features.json
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ app.py                   # App
â”‚       â”œâ”€â”€ config.py                # Configuration
â”‚       â”œâ”€â”€ inference.py             # Prediction Logic
â”‚       â””â”€â”€ pipeline.py              # Training Pipeline
â”‚
â””â”€â”€ OIG2.png
```

## 6. Validation Strategy
We used **Stratified K-Fold Cross-Validation** to ensure our model generalizes well across all credit score classes, preventing overfitting to the "Standard" class which is the majority.

## 7. Pipeline Strategy
*   **Preprocessing**: robust regex cleaning for dirty numerical columns.
*   **Imputation**: Median imputation for skewed financial data.
*   **Model**: XGBoost chosen for its ability to handle non-linear relationships and high performance on tabular data.

## 8. Monitoring
Post-deployment, we recommend monitoring:
*   **Accuracy**: Check against ground truth labels after 3 months.
*   **Data Drift**: Monitor `Annual_Income` and `Debt` distributions for shifts.

## ðŸ“Œ To-Do: Business & Model Improvements

- [ ] Validate the final model on a separate holdout test set  
- [ ] Set up model monitoring (monthly accuracy, drift in key features)  
- [ ] Define decision thresholds for each credit score class  
- [ ] Add fallback rules for uncertain predictions (e.g., probability < 55%)  
- [ ] Build a feedback loop to compare predicted vs actual scores  
- [ ] Document model limitations and train credit team on edge cases 

## Contact
*   **Author**: [Your Name]
*   **Email**: [Your Email]
*   **LinkedIn**: [Your Profile]
